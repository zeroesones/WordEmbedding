
import pandas as pd

# Create a DataFrame with 10 rows of sample data for loan applicants
data = {
    'Annual_Income': [50000, 75000, 100000, 120000, 90000, 60000, 55000, 65000, 70000, 110000],
    'Credit_Score': [700, 750, 800, 650, 720, 690, 710, 730, 760, 790],
    'Employment_Length': [5, 10, 2, 8, 4, 6, 1, 7, 9, 3],
    'Debt_to_Income_Ratio': [0.2, 0.3, 0.1, 0.4, 0.15, 0.25, 0.35, 0.22, 0.18, 0.28],
    'Loan_Amount_Requested': [20000, 30000, 10000, 50000, 15000, 25000, 7000, 12000, 18000, 22000],
    'Loan_Purpose': ['Home', 'Debt Consolidation', 'Education', 'Business', 'Travel', 'Medical', 'Other', 'Home', 'Business', 'Education'],
    'Home_Ownership_Status': ['Own', 'Mortgage', 'Rent', 'Own', 'Rent', 'Mortgage', 'Rent', 'Own', 'Mortgage', 'Rent'],
    'Number_of_Open_Accounts': [4, 3, 6, 5, 2, 7, 4, 3, 5, 6],
    'Number_of_Late_Payments': [0, 1, 0, 2, 1, 0, 1, 0, 0, 1]
}

df = pd.DataFrame(data)

# Display the DataFrame
print(df)





Sub CreateFeatureEngineeringPresentation()

    Dim pptApp As Object
    Dim pptPresentation As Object
    Dim pptSlide As Object
    
    ' Initialize PowerPoint application
    Set pptApp = CreateObject("PowerPoint.Application")
    pptApp.Visible = True
    
    ' Add a new Presentation
    Set pptPresentation = pptApp.Presentations.Add
    
    ' Slide 1: Title Slide
    Set pptSlide = pptPresentation.Slides.Add(1, ppLayoutTitle)
    pptSlide.Shapes(1).TextFrame.TextRange.Text = "Feature Engineering and Extraction"
    pptSlide.Shapes(2).TextFrame.TextRange.Text = "An In-depth Overview and Hands-on Session"
    
    ' Slide 2: Introduction
    Set pptSlide = pptPresentation.Slides.Add(2, ppLayoutText)
    pptSlide.Shapes(1).TextFrame.TextRange.Text = "Part I: Theoretical Understanding"
    pptSlide.Shapes(2).TextFrame.TextRange.Text = "This section will cover: " & vbCrLf & _
                                                 "1. What is Feature Engineering?" & vbCrLf & _
                                                 "2. Types of Features" & vbCrLf & _
                                                 "3. Feature Extraction vs Feature Engineering" & vbCrLf & _
                                                 "4. Case Studies" & vbCrLf & _
                                                 "5. Common Techniques"
    
    ' Slide 3: What is Feature Engineering?
    Set pptSlide = pptPresentation.Slides.Add(3, ppLayoutText)
    pptSlide.Shapes(1).TextFrame.TextRange.Text = "What is Feature Engineering?"
    pptSlide.Shapes(2).TextFrame.TextRange.Text = "Definition: The process of transforming raw data into a format that is better suited for machine learning models." & vbCrLf & _
                                                 "Importance: Better features lead to better model performance." & vbCrLf & _
                                                 "Requires: Often requires domain expertise to create effective features." & vbCrLf & _
                                                 "Goal: Improve model performance, and reduce computational or data needs."
                                                 
    ' Slide 4: Types of Features
    Set pptSlide = pptPresentation.Slides.Add(4, ppLayoutText)
    pptSlide.Shapes(1).TextFrame.TextRange.Text = "Types of Features"
    pptSlide.Shapes(2).TextFrame.TextRange.Text = "Numerical: Represented by integers or real numbers. Example: Age, Salary." & vbCrLf & _
                                                 "Categorical: Discrete set of values or labels. Example: Colors, Brands." & vbCrLf & _
                                                 "Ordinal: Ordered categories. Example: Ratings (High, Medium, Low)." & vbCrLf & _
                                                 "Text: Unstructured data that needs NLP techniques for feature extraction." & vbCrLf & _
                                                 "Time-Series: Ordered data points collected over time. Useful in forecasting." & vbCrLf & _
                                                 "Images: Pixel data, requires techniques like convolution for feature extraction."
    ' Add Python code for one-hot encoding of Categorical Data
    Set pptSlide = pptPresentation.Slides.Add(5, ppLayoutText)
    pptSlide.Shapes(1).TextFrame.TextRange.Text = "Python Code: One-hot Encoding for Categorical Data"
    pptSlide.Shapes(2).TextFrame.TextRange.Text = "from sklearn.preprocessing import OneHotEncoder" & vbCrLf & _
                                                 "encoder = OneHotEncoder()" & vbCrLf & _
                                                 "encoded_features = encoder.fit_transform(df[['color']]).toarray()"
    
    ' Continue with remaining slides using similar format
    
End Sub






Slide 6: Feature Extraction vs Feature Engineering
Content:

"Feature Extraction aims to reduce dimensionality or extract useful information."
"Feature Engineering requires domain expertise to create new features."
"Feature Extraction is more automated while Feature Engineering can be manual."
"Python libraries like PCA for Feature Extraction and scikit-learn for Feature Engineering."
Slide 7: Python Code Sample for Feature Extraction using PCA
Content:

scss
Copy code
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
Slide 8: Case Studies: Importance in Real-world Applications
Content:

"Finance: Fraud detection algorithms require engineered features like transaction patterns."
"Healthcare: Predicting patient readmissions with historical data and vitals."
"Retail: Personalization engines use features like user behavior and product metadata."
Slide 9: Common Techniques in Feature Engineering
Content:

"Transformation: Converting data into another format. E.g., log transformation."
"Normalization: Scaling features to have a similar range."
"Binning: Grouping continuous features into intervals."
"Encoding: Transforming categorical features into a numerical format."
Slide 10: Python Code Sample for Binning and Encoding
Content:

bash
Copy code
# Binning
df['Age_bin'] = pd.cut(df['Age'], bins=[20, 40, 60, 80])

# Encoding
df['Gender_encoded'] = df['Gender'].map({'Male': 1, 'Female': 0})
Slide 11: Part II: Practical Session
Content:

"Hands-on activities for feature engineering."
"Work with real-world datasets."
"Python Libraries: scikit-learn, pandas, NumPy."
Slide 12: Tools and Libraries for Feature Engineering
Content:

"Scikit-learn: Provides a wide range of feature engineering techniques."
"Pandas: Useful for data manipulation tasks."
"NumPy: Provides support for numerical operations."
Slide 13: Hands-on: Feature Engineering on Numerical Data
Content:

"Scaling: Min-Max Scaling, Standard Scaling."
"Python Code Sample:"
lua
Copy code
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['Age', 'Salary']] = scaler.fit_transform(df[['Age', 'Salary']])
Slide 14: Hands-on: Feature Engineering on Categorical Data
Content:

"One-Hot Encoding, Label Encoding."
"Python Code Sample:"
css
Copy code
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df['Gender'] = encoder.fit_transform(df['Gender'])
Slide 15: Hands-on: Text Feature Extraction
Content:

"Techniques: TF-IDF, Word Embeddings."
"Python Code Sample:"
scss
Copy code
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
Slide 16: Q&A for Practical Session
Content:

"Open for any questions related to the practical session."
Slide 17: Part III: Advanced Topics
Content:

"Automated Feature Engineering."
"Feature Selection Techniques."
"Pitfalls and Best Practices."
Slide 18: Automated Feature Engineering
Content:

"Libraries like Featuretools can automatically create new features."
"Python Code Sample:"
python
Copy code
import featuretools as ft
es = ft.EntitySet(id = 'data')
es = es.entity_from_dataframe(entity_id='data_id', dataframe=df, ...)
features, feature_defs = ft.dfs(entityset=es, target_entity='data_id')
Slide 19: Feature Selection Techniques
Content:

"Methods include Recursive Feature Elimination, Feature Importance."
"Python Code Sample:"
scss
Copy code
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(model, 3)
fit = rfe.fit(X, Y)
Slide 20: Pitfalls and Best Practices
Content:

"Overfitting: Creating too many features might lead to overfitting."
"Relevance: Not all created features are useful."
"Data leakage: Make sure the feature doesn't include information from the target variable."
You can add these contents to your VBA script following the pattern established for the earlier slides. Each slide will now have between 5-10 lines or bullet points of relevant content along with Python code samples where applicable





You can use the Pandas library in Python to read multiple sheets from an Excel workbook and then consolidate the information into a single DataFrame (i.e., a single sheet). You can then write this consolidated DataFrame back into a new Excel sheet.

Here's a step-by-step example:

1. Read each sheet into its own DataFrame.
2. Identify the unique IDs and create a consolidated DataFrame with those IDs.
3. Populate the attributes from each sheet into the consolidated DataFrame.

```python
import pandas as pd

# Step 1: Read each sheet into its own DataFrame
xls = pd.ExcelFile("your_workbook.xlsx")
sheet1 = pd.read_excel(xls, 'Sheet1')
sheet2 = pd.read_excel(xls, 'Sheet2')
# Add as many sheets as you have

# Step 2: Identify unique IDs and create a consolidated DataFrame
unique_ids = pd.concat([sheet1['ID'], sheet2['ID']]).drop_duplicates().reset_index(drop=True) # Add more sheets as needed
consolidated_df = pd.DataFrame({'ID': unique_ids})

# Step 3: Populate attributes from each sheet into the consolidated DataFrame
def populate_attribute(sheet, attribute_column, consolidated_df):
    attribute_dict = sheet.set_index('ID')[attribute_column].to_dict()
    consolidated_df[attribute_column] = consolidated_df['ID'].map(attribute_dict)

# Populate 'attribute1' from 'Sheet1'
populate_attribute(sheet1, 'attribute1', consolidated_df)

# Populate other attributes from other sheets as needed
# populate_attribute(sheet2, 'attribute2', consolidated_df)

# Write the consolidated DataFrame to a new Excel sheet
consolidated_df.to_excel("consolidated_sheet.xlsx", index=False)
```

In this example, replace `your_workbook.xlsx` with the name of your Excel workbook. Also, replace `'Sheet1'`, `'Sheet2'`, etc., with the names of your sheets, and `'attribute1'` with the names of the attribute columns you're interested in.

Remember to install pandas and openpyxl libraries for this to work. You can install them via pip:
```bash
pip install pandas openpyxl
```

This code should give you a new Excel file, `consolidated_sheet.xlsx`, with the consolidated information.


You can update the regular expression pattern to account for variations like "Bank" or "Counterparty" and "Party B" or "Counterparty". Here's how you can modify the code:

```python
import pandas as pd
import re

# Sample DataFrame
df = pd.DataFrame({
    'text': [
        'Some text "Valuation Date Location" means, with respect to each party, each city, region or country specified below: Party A: London Party B: Copenhagen Some more text',
        'Another text "Valuation Date Location" means, with respect to: Bank: New York Party B: London',
        'Yet another text "Valuation Date Location" means, with respect to: Counterparty: Tokyo Party B: Sydney'
    ]
})

# Function to clean the text
def clean_text(text):
    return re.sub(r'[,"":]', '', text)

# Function to extract the complete information about Valuation Date Location
def extract_full_info(text):
    regex_pattern = r'"Valuation Date Location" means, with respect to.*?((Party [AB]|Bank|Counterparty): [\w\s]+[\s]*)+'
    match = re.search(regex_pattern, text)
    return match.group(0) if match else None

# Clean the text
df['cleaned_text'] = df['text'].apply(clean_text)

# Extract complete information
df['extracted_info'] = df['cleaned_text'].apply(extract_full_info)

print(df[['text', 'extracted_info']])
```

In this modified version, the regular expression now includes "Bank" or "Counterparty" and "Party [AB]" or "Counterparty", allowing it to match any of these options when they appear in the sentence. The `extracted_info` column will then contain the complete information, including these variations.



You can use the Python library `pandas` along with regular expressions (`re`) to extract the specified phrases from a DataFrame column that contains text. Here's a simple example code snippet:

```python
import pandas as pd
import re

# Sample DataFrame
df = pd.DataFrame({
    'text': [
        'Some text "Valuation Date Location" means, with respect to each party, each city, region or country specified below: Party A: London Party B: Copenhagen Some more text',
        'Another text "Valuation Date Location" means, with respect to: Bank: New York Party B: London'
    ]
})

# Function to extract the matching phrase
def extract_location(text):
    regex_pattern = r'"Valuation Date Location" means, with respect to.*?(Party [AB]: [\w\s]+|Bank: [\w\s]+)+'
    match = re.search(regex_pattern, text)
    return match.group(0) if match else None

# Apply the function to the DataFrame
df['extracted_text'] = df['text'].apply(extract_location)

print(df)
```

The new column `'extracted_text'` will contain the matching phrases you're looking for.

Note: The regular expression I used (`regex_pattern`) is just a sample and might not capture all variations of the text structure you provided. You may need to modify it according to your exact needs.



To extract city or country names from each sentence, you can use regular expressions along with the `re.findall()` function. The function will return all non-overlapping matches of the pattern in the string as a list. Here's how you can do it:

```python
import pandas as pd
import re

# Sample DataFrame
df = pd.DataFrame({
    'text': [
        'Some text "Valuation Date Location" means, with respect to each party, each city, region or country specified below: Party A: London Party B: Copenhagen Some more text',
        'Another text "Valuation Date Location" means, with respect to: Bank: New York Party B: London'
    ]
})

# Function to clean the text
def clean_text(text):
    return re.sub(r'[,"":]', '', text)

# Function to extract cities or countries
def extract_location(text):
    matches = re.findall(r'(Party [AB]|Bank): ([\w\s]+)', text)
    return [match[1] for match in matches]

# Clean the text
df['cleaned_text'] = df['text'].apply(clean_text)

# Extract cities or countries
df['extracted_locations'] = df['cleaned_text'].apply(extract_location)

print(df[['text', 'extracted_locations']])
```

This will add a new column `extracted_locations` to the DataFrame, containing the extracted cities or countries as lists. For sentence 1, it will contain `['London', 'Copenhagen']`, and for sentence 2, it will contain `['New York', 'London']`.




pattern = r'(?:Party A\s*)?([A-Za-z]+|\$|€|£)?\s*(\d+(?:\.\d{0,2})?)'


def check_values(row):
    if pd.notna(row['C']):
        values = row['C'].split()
        values = [float(v) / 1000 for v in values if v != 'NaN']
        return any(np.isclose(values, row['D'], atol=1e-6))
    return False


import pandas as pd
import numpy as np

# Sample data
data = {'C': ['0.5 1.2 3.0', '2.5', '1.0', np.nan],
        'D': [12, 25, 10, 20]}
df = pd.DataFrame(data)

# Function to check if any value in C matches with D
def check_match(row, col_c, col_d):
    if pd.notna(row[col_c]):
        c_value = row[col_c]
        if isinstance(c_value, str):
            values = c_value.split()
            for val in values:
                if pd.notna(val):
                    try:
                        float_val = float(val) / 1000
                        if np.isclose(float_val, row[col_d]):
                            return True
                    except ValueError:
                        pass
        else:
            try:
                float_val = float(c_value) / 1000
                if np.isclose(float_val, row[col_d]):
                    return True
            except ValueError:
                pass
    return False

# Columns to compare
col_c = 'C'
col_d = 'D'

# Apply the function to each row and create a new column 'Result'
df['Result'] = df.apply(lambda row: check_match(row, col_c, col_d), axis=1)

print(df)




import pandas as pd
import numpy as np

# Sample data
data = {'C': ['0.5 1.2 3.0', '2.5', '1.0', np.nan],
        'D': [12, 25, 10, 20]}
df = pd.DataFrame(data)

# Function to check if any value in C matches with D
def check_match(row, col_c, col_d):
    if pd.notna(row[col_c]):
        if isinstance(row[col_c], str):
            values = row[col_c].split()
        else:
            values = [str(row[col_c])]
        for val in values:
            if pd.notna(val):
                try:
                    float_val = float(val) / 1000
                    if np.isclose(float_val, row[col_d]):
                        return True
                except ValueError:
                    pass
    return False

# Columns to compare
col_c = 'C'
col_d = 'D'

# Apply the function to each row and create a new column 'Result'
df['Result'] = df.apply(lambda row: check_match(row, col_c, col_d), axis=1)

print(df)




import pandas as pd
import numpy as np

# Sample data
data = {'C': ['0.5 1.2 3.0', '2.5', '1.0', np.nan],
        'D': [12, 25, 10, 20]}
df = pd.DataFrame(data)

# Function to check if any value in C matches with D
def check_match(row, col_c, col_d):
    if pd.notna(row[col_c]):
        values = row[col_c].split()
        for val in values:
            if pd.notna(val):
                try:
                    float_val = float(val) / 1000
                    if np.isclose(float_val, row[col_d]):
                        return True
                except ValueError:
                    pass
    return False

# Columns to compare
col_c = 'C'
col_d = 'D'

# Apply the function to each row and create a new column 'Result'
df['Result'] = df.apply(lambda row: check_match(row, col_c, col_d), axis=1)

print(df)





Certainly! You can remove the commas from the text before applying the regular expression. Additionally, you can update the regular expression to account for the optional space between the currency and the amount. Here's the updated code:

```python
import pandas as pd
import re

# Sample dataframe
data = {'date': ['2021-01-01', '2021-01-02'],
        'id': [1, 2],
        'dogref': ['ref1', 'ref2'],
        'text': ['minimum transfer amount with respect to Party A, USD 10,000',
                 'minimum transfer amount with respect to Party A and Party B $20,000']}

df = pd.DataFrame(data)

# Function to extract currency and amount
def extract_currency_amount(text):
    text = text.replace(",", "")  # Remove commas
    match = re.search(r'(?:[A-Za-z]{3}|\$|€|£)\s*(\d{1,3}(?:[\.]\d{0,2})?)', text)
    if match:
        currency = match.group(1).strip()
        amount = match.group(2)
        return currency, float(amount)
    return None, None

# Apply the function and create new columns
df['currency'], df['amount'] = zip(*df['text'].apply(extract_currency_amount))

print(df)
```

This updated code should meet your new conditions:

- Commas are removed from the text before applying the regex.
- The regex allows for an optional space between the currency and the amount.


o account for the possibility of a space or no space between the currency symbol and the amount, you can update the regular expression pattern. The "\s?" denotes that there can be zero or one whitespace character. Here's how you can modify the code:

```python
import pandas as pd
import re

# Sample DataFrame (you would use your real DataFrame here)
data = {
    'MID': [1, 2],
    'DOCREFID': ['Doc1', 'Doc2'],
    'AGREEMENT CONTENT': [
        'MINIMUM TRANSFER AMOUNT means with respect to a party as of any date USD $100,000.',
        'MINIMUM TRANSFER AMOUNT means with respect to a party as of any date EUR€50,000.'
    ]
}

df = pd.DataFrame(data)

# Initialize new columns
df['Extracted_Phrase'] = ''

# Regular expression pattern to match the relevant phrases
pattern = r"(MINIMUM TRANSFER AMOUNT means with respect to a party as of any date [\w\$€£¥]\s?[\d,]+)"

def extract_phrase(row):
    match = re.search(pattern, row['AGREEMENT CONTENT'], re.IGNORECASE)
    if match:
        row['Extracted_Phrase'] = match.group(0)
    return row

# Apply the function to each row
df = df.apply(extract_phrase, axis=1)

print(df)
```

In this updated pattern, the "\s?" will match either a space or no space between the currency symbol and the amount, catering to both scenarios.






If you prefer to store multiple values separated by spaces in the same cell rather than as lists, you can adapt the code to concatenate the extracted values into strings. Here's how you can do it:

```python





import pandas as pd
import re

# Sample DataFrame
data = {
    'Primary Key': [1, 2],
    'Reference Number': ['Ref1', 'Ref2'],
    'Actual Text': [
        "Some text here and 'minimum net asset amount' with subject to party A. Another 'minimum net asset amount' with subject to party B, USD $30,000.",
        '"minimum net asset amount" with respect to party A and party B, USD $20,000. Also, minimum net asset amount with respect to party B, €25,000.'
    ]
}

df = pd.DataFrame(data)

# Initialize empty columns with default empty strings
df['Patterns'] = ''
df['Parties'] = ''
df['Additional Parties'] = ''
df['Currencies'] = ''
df['Amounts'] = ''

# Regular expression pattern
pattern = r"(['\"]?)minimum net asset amount(['\"]?) (?:with (?:subject to|respect to) )?party ([A,B])(?: and party ([A,B]))?,? ([\w\$\€\£\¥]{1,4})?\s?([\d,]+)?"

def extract_info(row):
    matches = re.findall(pattern, row['Actual Text'], re.IGNORECASE)
    
    for match in matches:
        row['Patterns'] += ' '.join(match) + ' '
        row['Parties'] += match[3] + ' '
        row['Additional Parties'] += (match[4] + ' ') if match[4] else ''
        row['Currencies'] += (match[5].strip() + ' ') if match[5].strip() else 'USD '
        row['Amounts'] += match[6].replace(',', '') + ' ' if match[6] else ''

    return row

# Apply the function to each row
df = df.apply(extract_info, axis=1)

# Trim the trailing spaces
for col in ['Patterns', 'Parties', 'Additional Parties', 'Currencies', 'Amounts']:
    df[col] = df[col].str.rstrip()

print(df)
```

In this version, the new columns "Patterns", "Parties", "Additional Parties", "Currencies", and "Amounts" contain strings that hold all relevant extracted values for each row in the DataFrame, separated by spaces. After processing, the code also trims any trailing spaces.





To accomplish this, you can modify the regular expression pattern and the Python code to tag the amount based on whether it's for party A, party B, or both. You'll need to capture the party (either "A" or "B") mentioned in the phrase and use it to tag the amount accordingly.

Here's the updated code:

```python
import re

def extract_phrases(text):
    pattern = r"(['\"]?)minimum net asset amount(['\"]?) (?:with (?:subject to|respect to) )?party ([A,B])(?: and party ([A,B]))?,? ([\w\$\€\£\¥]{1,4})?\s?([\d,]+)?"
    matches = re.findall(pattern, text, re.IGNORECASE)

    for match in matches:
        full_match = ''.join(match)
        print("Found:", full_match)

        party = match[3]  # Party A or B
        additional_party = match[4]  # If there's a second party mentioned
        currency = match[5].strip() if match[5].strip() else 'USD'  # Default to USD if not specified
        amount = match[6].replace(',', '')  # Removing commas for easier parsing

        # Determine party tag
        if additional_party:
            tag = f"Amount common to party {party} and party {additional_party}"
        else:
            tag = f"Amount specific to party {party}"

        print(tag)
        print(f"Currency: {currency}")
        print(f"Amount: {amount}")

# Sample data
sample_text = """
Page 1: Some text here and 'minimum net asset amount' with subject to party A.
Page 2: Another text, "minimum net asset amount" with respect to party A and party B, USD$20,000.
Page 3: Yet another text, minimum net asset amount with respect to party A, $45,000.
Page 4: In Europe, minimum net asset amount with respect to party B, €25,000.
"""

extract_phrases(sample_text)
```

In this updated code, I modified the regular expression to capture the party (either "A" or "B") using `party ([A,B])(?: and party ([A,B]))?,?`. Then, within the Python code, I use these captured values to determine if the amount is specific to party A, party B, or common to both, and tag it accordingly.



import pandas as pd
import re
from datetime import datetime
import datefinder

# Sample data
data = {'ContractDate': ['Agreement dated as o May 19, 2008',
                         'Master Agreement dated as of 9th July 2004',
                         'Agreement A dated as o 10th July 2008 B dated as of 10 October 2022 C dated as o October 312008',
                         'Amendment dated as of January 24, 20! 8']}
df = pd.DataFrame(data)

# Function to extract and clean dates
def extract_clean_date(text):
    # Remove noise characters
    cleaned_text = re.sub(r'20! 8', '2018', text)
    cleaned_text = re.sub(r'[^0-9a-zA-Z\s]', '', cleaned_text)
    
    # Replace '20! 8' with '2018'
    
    print(cleaned_text)

    # Use datefinder to extract dates
    matches = list(datefinder.find_dates(cleaned_text, strict=True))
    print(matches)
    if matches:
        # Get the first matched date and format it
        extracted_date = matches[0]
        return extracted_date.strftime('%Y-%m-%d')
    
    return None

# Apply the function to the 'ContractDate' column
df['CleanedDate'] = df['ContractDate'].apply(extract_clean_date)

# Display the cleaned dates
print(df)
